---
title: Sources of uncertainty
date: 2025-04-19
categories: [Introduction]
tags: []  
math: true
---

When trying to quantify the uncertainty of the predictions of a Machine Learning (ML) model, it is useful to understand what the different sources of uncertainty are. Suppose we are given data $$\{(X_i,y_i)\}$$ (i.e. *features*, *target* pairs) which have been generated by the following process:

$$
\begin{equation}
y = f(X) + \epsilon
\end{equation}
$$ 

Here $$f$$ is the actual but unkown deterministc model and $$\epsilon$$ is unobserved noise due to randomness. The task of ML is to $$\hat{f}$$ given sample data $$\{(X_i,y_i)\}$$ which approximates the unknown $$f$$ as good as possible. The pressence of the random noise, can make this a formidable task. 

Once we have found a model $$\hat{f}$$ and make predictions with for a new $$X_n$$ we might want to quantify the uncertainty of this prediction. The prediction error can be (theortically) decomposed into two terms:

$$
\begin{equation}
\hat{f}(X_n) - y_n = \underbrace{\hat{f}(X_n) - f(X_n)}_{\text{model uncertainty}
} - \underbrace{\epsilon_n}_{\text{data uncertainty}}
\end{equation}
$$

We see that the uncertainty is a combination of *model uncertainty* and *data uncertainty*.
The model uncertainty, is also called epistemic (aka systematic) uncertainty. In theory this uncertainty can be reduced to zero given enough training data, sufficient training time and using model family which contains the true model $$f$$. 

The second type, data uncertainty, is also called aleatoric (aka statistical) uncertainty. The data uncertainty is due to the inherit randomness and is irreducible. When training a model and we start to overfit our model is trying to makes sense of the randomness.

While it is clear that in most ML problems we (indirectly) have to deal with these two sources of uncertainty, it is not always obvious where to draw the line between the two. For example we cannot know for sure that $$f$$ and $$X$$ describe the deterministic part of the problem completly, there could be a (possible unknown) richer feature set $$\tilde{X}$$ and corresponding a function $$\tilde{f}$$ which explain some of the originally assumed data uncertainty.

## Illustration of model and data uncertainty
In this simplified example, we assume that our model family contains the true function $$f$$. When we increase the number of samples we see that the *model uncertainty* gradually goes to zero while the *data uncertainty* and as a result the *total uncertainty* converges to a finite value.

{% include plotly_plots/uncertainty_sources.html%}
*Move the slider to see what the effect of increasing the sample size is on the model and data uncertainty.*

## Further Reading
* [*Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods* - E. HÃ¼llermeier & W. Waegeman (2021)](https://link.springer.com/article/10.1007/s10994-021-05946-3)

<!-- how do we estimate it -->
<!-- what does it mean for fitting a model -->