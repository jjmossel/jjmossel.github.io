---
title: Sources of uncertainty
date: 2025-04-19
categories: [Introduction]
tags: []  
math: true
---

When trying to quantify the uncertainty of the predictions of a Machine Learning (ML) model, it is useful to understand what the different sources of uncertainty are. In short there are two types of uncertainty, one that can be reduced and one that cannot. The reducible uncertainty, also called *model uncertainty* or *epistemtic uncertainty*, is the uncertainty due to a lack of knowledge. Either because of a misspecification of the model, or in the estimation of the model parameters. In principle, given enough data and resources the model uncertainty can be reduced to zero. The irrediducle uncertainty, is a result of stochastic processes, is also called *aleateoric uncertainty* or *data uncertainty*.

To make this more concrete, consider the following regression problem. We are given data $$\{(X_1,y_1),\ldots,(X_n,y_n)\}$$ (i.e. *features*, *target* pairs) which have been generated by the following process:

$$
\begin{equation}
y = f(X) + \epsilon(X)
\end{equation}
$$ 

Here $$f$$ is the actual but unkown deterministc model and $$\epsilon$$ is unobserved noise due to randomness. The task of ML is to find a $$\hat{f}$$ given the sample data which approximates the unknown $$f$$ as good as possible. The pressence of the random noise, can make this a formidable task. 

Once we have found a model $$\hat{f}$$, we typically want to make predictions with if for a new $$X_{n+1}$$. For this regression problem we can decompose, in theory, the prediction error into two terms:

$$
\begin{equation}
\hat{f}(X_{n+1}) - y_{n+1} = \underbrace{\hat{f}(X_{n+1}) - f(X_{n+1})}_{\text{model uncertainty}
} - \underbrace{\epsilon_{n+1}}_{\text{data uncertainty}}
\end{equation}
$$

We see that the uncertainty is a combination of *model uncertainty* and *data uncertainty*. As mentioned earlier the model uncertainty can, in theory, be reduced to zero if our model $$\hat{f}$$ approaces the true (but hidden) mdoel $$f$$. The data uncertainty is random noise and cannot be redudced.

While it is clear that in most ML problems we (indirectly) have to deal with these two sources of uncertainty, it is not always obvious where to draw the line between the two. For example we cannot know for sure that $$f$$ and $$X$$ describe the deterministic part of the problem completly, there could be a (possible unknown) richer feature set $$\tilde{X}$$ and corresponding a function $$\tilde{f}$$ which explain some of the originally assumed data uncertainty.

## Illustration of model and data uncertainty
In this simplified example, we assume that our model family contains the true function $$f$$. When we increase the number of samples we see that the *model uncertainty* gradually goes to zero while the *data uncertainty* and as a result the *total uncertainty* converges to a finite value.

{% include plotly_plots/uncertainty_sources.html%}
*Move the slider to see what the effect of increasing the sample size is on the model and data uncertainty.*

The above example was created based on a ordindary least squares problem, see [Linear Regression Example]({% post_url 2025-04-25-linear_regression %}).

## Further Reading
* [*Aleatoric and epistemic uncertainty in machine learning: an introduction to concepts and methods* - E. HÃ¼llermeier & W. Waegeman (2021)](https://link.springer.com/article/10.1007/s10994-021-05946-3)

* [*What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?* - A. Kendall, Y. Gal (2017)](https://arxiv.org/abs/1703.04977)

<!-- how do we estimate it -->
<!-- what does it mean for fitting a model -->